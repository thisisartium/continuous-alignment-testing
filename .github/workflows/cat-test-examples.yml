name: CAT

on:
  push:
    branches: [ ci-experiment/** ]
  workflow_dispatch:
    inputs:
      rounds:
        description: "Number of Rounds 1 - 128"
        type: number
        required: true
        default: 10

jobs:
  ai_tests:
    name: AI Tests
    runs-on: ubuntu-latest
    env:
      TEST_RESULTS_FOLDER: examples/team_recommender/test_runs

    steps:
      - uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v5
        with:
          enable-cache: true
          prune-cache: false

      - name: "Set up Python"
        uses: actions/setup-python@v5
        with:
          python-version-file: "pyproject.toml"

      - name: Install the project
        run: uv sync --all-extras --dev

      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v2
        with:
          credentials_json: '${{ secrets.GOOGLE_CREDENTIALS }}'

      - name: Set number of runs
        id: set-number-of-runs
        run: |
          ROUNDS=${{ inputs.rounds || 10 }}
          [[ $GITHUB_REF_NAME == ci-experiment* ]] && ROUNDS=1

          echo "::notice::Starting ${ROUNDS} run$([ "$ROUNDS" -eq 1 ] || echo "s")"
          echo "number_of_runs=$ROUNDS" >> "$GITHUB_OUTPUT"
          echo "CAT_AI_SAMPLE_SIZE=$ROUNDS" >> $GITHUB_ENV

      - name: Run Example tests
        run: >
          uv run pytest
          --verbose --verbosity=10 --capture=no --tb=native --color=yes --showlocals
          examples/team_recommender/tests/example_7_*
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}

#      - name: Upload artifacts to MinIO
#        run: |
#          zip -r test-output-${{ github.run_number }}.zip "examples/team_recommender/test_runs"
#            curl -X PUT -T "/path/to/yourfile.zip" \
#            -H "Host: localhost:9000" \
#            -H "Date: $(date -R)" \
#            -H "Content-Type: application/zip" \
#            -H "Authorization: AWS minioadmin:minioadmin" \
#            http://localhost:9000/yourbucket/yourfile.zip

      - name: Show CAT AI Statistical Report
        if: always()
        run: |          
          FAILURE_COUNT=$(find "$TEST_RESULTS_FOLDER" -type f -name "fail-*.json" | wc -l)
          PYTHONPATH=src uv run python -m cat_ai.reporter \
            "$FAILURE_COUNT" \
            "$CAT_AI_SAMPLE_SIZE" \
            >> "$GITHUB_STEP_SUMMARY"

      - name: Upload main artifacts to Google Drive
        if: always() && github.ref == 'refs/heads/main'
        run: |
          zip -r "$ZIP_WITH_RUN" "$TEST_RESULTS_FOLDER"
          uv run python src/cat_ai/publish_to_gdrive.py "$ZIP_WITH_RUN"
        env:
          PARENT_FOLDER_IDS: ${{ vars.GOOGLE_DRIVE_TEST_OUTPUT_FOLDER_ID }}
          ZIP_WITH_RUN: test-output-${{ github.run_number }}.zip

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        if: always()
        with:
            name: test-output-${{ github.run_number }}
            path: ${{ env.TEST_RESULTS_FOLDER }}

#      - name: Debugging with tmate
#        if: failure()
#        uses: lhotari/action-upterm@v1
#        with:
#          wait-timeout-minutes: 5
